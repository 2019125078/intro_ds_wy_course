{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_linear_data, create_linear_model, create_summary_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, Y, model, learning_rate=0.01,\n",
    "                                mini_batch_fraction=0.01, epoch=10000, tol=1.e-6):\n",
    "    \"\"\"\n",
    "    利用随机梯度下降法训练模型。\n",
    "    参数\n",
    "    ----\n",
    "    X : np.array, 自变量数据\n",
    "    Y : np.array, 因变量数据\n",
    "    model : dict, 里面包含模型的参数，损失函数，自变量，应变量\n",
    "    \"\"\"\n",
    "    # 确定最优化算法\n",
    "    ## 普通随机梯度下降法\n",
    "    ## algo_name = \"stochastic_gradient_descent\"\n",
    "    ## method = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    ## Adam算法\n",
    "    ## algo_name = \"adam\"\n",
    "    ## method = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    ## Momentum算法\n",
    "    algo_name = \"momentum\"\n",
    "    method = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "    optimizer = method.minimize(model[\"loss_function\"])\n",
    "    # 增加日志\n",
    "    tf.summary.scalar(\"loss_function\", model[\"loss_function\"])\n",
    "    tf.summary.histogram(\"params\", model[\"model_params\"])\n",
    "    tf.summary.scalar(\"first_param\", tf.reduce_mean(model[\"model_params\"][0]))\n",
    "    tf.summary.scalar(\"last_param\", tf.reduce_mean(model[\"model_params\"][-1]))\n",
    "    summary = tf.summary.merge_all()\n",
    "    # 在程序运行结束之后，运行如下命令，查看日志\n",
    "    # tensorboard --logdir logs/\n",
    "    # Windows下的存储路径与Linux并不相同\n",
    "    if os.name == \"nt\":\n",
    "        summary_writer = create_summary_writer(\"logs\\\\%s\" % algo_name)\n",
    "    else:\n",
    "        summary_writer = create_summary_writer(\"logs/%s\" % algo_name)\n",
    "    # tensorflow开始运行\n",
    "    sess = tf.Session()\n",
    "    # 产生初始参数\n",
    "    init = tf.global_variables_initializer()\n",
    "    # 用之前产生的初始参数初始化模型\n",
    "    sess.run(init)\n",
    "    # 迭代梯度下降法\n",
    "    step = 0\n",
    "    batch_size = int(X.shape[0] * mini_batch_fraction)\n",
    "    batch_num = int(math.ceil(1 / mini_batch_fraction))\n",
    "    prev_loss = np.inf\n",
    "    diff = np.inf\n",
    "    # 当损失函数的变动小于阈值或达到最大训练轮次，则停止迭代\n",
    "    while (step < epoch) & (diff > tol):\n",
    "        for i in range(batch_num):\n",
    "            # 选取小批次训练数据\n",
    "            batch_x = X[i * batch_size: (i + 1) * batch_size]\n",
    "            batch_y = Y[i * batch_size: (i + 1) * batch_size]\n",
    "            # 迭代模型参数\n",
    "            sess.run([optimizer],\n",
    "                     feed_dict={model[\"independent_variable\"]: batch_x,\n",
    "                                model[\"dependent_variable\"]: batch_y})\n",
    "            # 计算损失函数并写入日志\n",
    "            summary_str, loss = sess.run(\n",
    "                [summary, model[\"loss_function\"]],\n",
    "                feed_dict={model[\"independent_variable\"]: X,\n",
    "                           model[\"dependent_variable\"]: Y})\n",
    "            # 将运行细节写入目录\n",
    "            summary_writer.add_summary(summary_str, step * batch_num + i)\n",
    "            # 计算损失函数的变动\n",
    "            diff = abs(prev_loss - loss)\n",
    "            prev_loss = loss\n",
    "            if diff <= tol:\n",
    "                break\n",
    "        step += 1\n",
    "    summary_writer.close()\n",
    "    # 在Windows下运行此脚本需确保Windows下的命令提示符(cmd)能显示中文\n",
    "    # 输出最终结果\n",
    "    print(\"模型参数：\\n%s\" % sess.run(model[\"model_params\"]))\n",
    "    print(\"训练轮次：%s\" % step)\n",
    "    print(\"损失函数值：%s\" % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型参数：\n",
      "[[ 1.08933102]\n",
      " [ 2.08670471]\n",
      " [ 3.05538793]\n",
      " [ 4.05002715]\n",
      " [ 5.08211534]\n",
      " [ 6.05396942]\n",
      " [ 7.05005748]\n",
      " [ 8.02663305]\n",
      " [ 9.06449827]\n",
      " [10.04979604]\n",
      " [11.07254313]\n",
      " [12.07415074]\n",
      " [13.04981054]\n",
      " [14.02282399]\n",
      " [15.05661438]\n",
      " [16.02721532]\n",
      " [17.02488773]\n",
      " [17.9978234 ]\n",
      " [19.04179644]\n",
      " [19.99594299]\n",
      " [21.01699086]\n",
      " [21.99119879]\n",
      " [23.00425656]\n",
      " [24.02718679]\n",
      " [24.99376572]\n",
      " [26.01620431]\n",
      " [26.99621739]\n",
      " [27.97025237]\n",
      " [28.98947651]\n",
      " [30.03365982]]\n",
      "训练轮次：3\n",
      "损失函数值：0.08650890333103624\n"
     ]
    }
   ],
   "source": [
    "# dimension表示自变量的个数，num表示数据集里数据的个数。\n",
    "dimension = 30\n",
    "num = 10000\n",
    "# 重置tensorflow\n",
    "tf.reset_default_graph()\n",
    "# 随机产生模型数据\n",
    "X, Y = generate_linear_data(dimension, num)\n",
    "# 定义模型\n",
    "model = create_linear_model(dimension)\n",
    "# 使用梯度下降法，估计模型参数\n",
    "stochastic_gradient_descent(X, Y, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
